{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction à l'apprentissage par renforcement\n",
    "# TP 1 - les manchots multi-bras\n",
    "\n",
    "1/4 de la note finale est liée à la mise en forme : \n",
    "\n",
    "* pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
    "* soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
    "* commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
    "\n",
    "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
    "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
    "\n",
    "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
    "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
    "\n",
    "```\n",
    "Nom1, Prenom1, Email1, NumEtudiant1\n",
    "Nom2, Prenom2, Email2, NumEtudiant2\n",
    "Nom3, Prenom3, Email3, NumEtudiant3\n",
    "Nom4, Prenom4, Email4, NumEtudiant4\n",
    "```\n",
    "\n",
    "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: ipympl in ./.venv/lib/python3.10/site-packages (0.9.4)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: ipython<9 in ./.venv/lib/python3.10/site-packages (from ipympl) (8.27.0)\n",
      "Requirement already satisfied: traitlets<6 in ./.venv/lib/python3.10/site-packages (from ipympl) (5.14.3)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in ./.venv/lib/python3.10/site-packages (from ipympl) (8.1.5)\n",
      "Requirement already satisfied: ipython-genutils in ./.venv/lib/python3.10/site-packages (from ipympl) (0.2.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (1.2.2)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (2.18.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (3.0.47)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (0.19.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (4.9.0)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (0.1.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.10/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.10/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.10/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython<9->ipympl) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython<9->ipympl) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython<9->ipympl) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-labextension` not found.\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-labextension` not found.\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib tqdm numpy ipympl opencv-python torch tqdm\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "!jupyter labextension install jupyter-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "import typing as t\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "K = 5 # num arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation du problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.testing\n",
    "\n",
    "\n",
    "class ArmBernoulli:\n",
    "    def __init__(self, p: float):\n",
    "        \"\"\"\n",
    "        Vaccine treatment following a Bernoulli law (mean is p and variance is p(1-p)\n",
    "        Args:\n",
    "             p (float): mean parameter\n",
    "             \n",
    "        >>> torch.random.manual_seed(random_state)        \n",
    "        >>> arm = ArmBernoulli(0.5)\n",
    "        >>> arm.sample(5)\n",
    "        tensor([ True, False,  True,  True,  True])\n",
    "        \"\"\"\n",
    "        self.immunity_rate = p\n",
    "\n",
    "    def sample(self, n: int = 1) -> torch.Tensor:\n",
    "        return torch.rand(n) < self.immunity_rate\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<ArmBernoulli p={self.immunity_rate}' \n",
    "\n",
    "def generate_arms(num_arms: int):\n",
    "    means = torch.rand(num_arms)\n",
    "    # print(\"means =\", means)\n",
    "    MAB = [ArmBernoulli(m) for m in means]\n",
    "    assert MAB[0].immunity_rate == means[0]\n",
    "    assert (MAB[0].sample(10) <= 1).all() and (MAB[0].sample(10) >= 0).all() \n",
    "    return MAB\n",
    "\n",
    "MAB = generate_arms(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP reprend l'exemple du médecin présenté en cours.\n",
    "\n",
    "\n",
    "**Q1. Créez une fonction pour trouver $\\mu^*$ à partir d'un `MAB`. Comment est définie la récompense $R_k$ ? Que représente concrètement le regret dans cet exemple ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_reward_from_best_vaccin(MAB : List[ArmBernoulli], hide_immunity_rate = False) -> float:\n",
    "\n",
    "    N = 1000\n",
    "    means : List[float]\n",
    "    \n",
    "    if hide_immunity_rate:\n",
    "        means = [torch.mean(arm.sample(N), dtype=float) for arm in MAB]\n",
    "    else:\n",
    "        means = [arm.immunity_rate for arm in MAB]\n",
    "    \n",
    "    print(\"means =\", means)\n",
    "    mu_star : float = max(means).item()\n",
    "    print(\"mu_star =\", mu_star)\n",
    "    return mu_star\n",
    "\n",
    "def apply_vaccin(arm : ArmBernoulli) -> int:\n",
    "    \"\"\"Rk\n",
    "    \n",
    "    Returns 1 if the patient is immunised thanks to the vaccin and 0 if not\"\"\"\n",
    "    return 1 if arm.sample(1).item() else 0\n",
    "\n",
    "def compute_regret(MAB : List[ArmBernoulli], N : int) -> float:\n",
    "    mu_star = find_mean_reward_from_best_vaccin(MAB, False)\n",
    "    # rn = N * mu_star - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means = [tensor(0.4963), tensor(0.7682), tensor(0.0885), tensor(0.1320), tensor(0.3074)]\n",
      "mu_star = 0.7682217955589294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7682217955589294"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_mean_reward_from_best_vaccin(MAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*\n",
    "\n",
    "**Que représente concrètement le regret dans cet exemple ?**\n",
    "\n",
    "[Réponse] Le regret représente le nombre de patient qui aurait pu être immunisé si le meilleur avait été utilisé à chaque fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note importante :** pour la suite, les tests seront faits avec 10 MAB différents ou plus pour réduire le bruit de simulation. Concrètement, on exécutera au moins 10x `generate_arms`.\n",
    "\n",
    "# I. Cas classique des bandits manchots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I.a. Solution Gloutonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le médecin fonctionne sur deux phases :\n",
    "\n",
    "1. **Exploration :** N patients reçoivent une dose d'un vaccin choisi aléatoirement. Le médecin calcule le taux d'immunisation empirique :\n",
    "\n",
    "$$\\bar{R_i} = \\frac{1}{T_i} \\sum_{k=0}^{N-1} \\chi_{v_k,i}R_k,$$\n",
    "\n",
    "avec $T_i = \\sum_{k=0}^{N-1} \\chi_{v_k,i}$.\n",
    "\n",
    "\n",
    "2. **Exploitation :** Le vaccin $v_i = \\arg\\max_j \\bar{R_j}$ est utilisé pour les M patients suivants. C'est la phase de test.\n",
    "\n",
    "**Q2. Implémentez la solution gloutonne avec N = 50 et M = 500 et testez la avec 100 MAB différents (tous ont 5 vaccins). On s'intéresse à la variable aléatoire \"la phase d'exploration a trouvé le bon vaccin\". Quelle est l'espérance empirique de cette variable ? Et son écart-type ? Calculez de même l'espérance et l'écart-type du regret sur vos 100 simulations.**\n",
    "\n",
    "Pour rappel, le regret est défini par :\n",
    "\n",
    "$$r_n = n\\mu^* - \\sum_{k=0}^{n-1} R_k$$\n",
    "\n",
    "**Attention :** $n$ est le nombre total de patients, donc ici $N + M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MABs = [generate_arms(5) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f287cc95d23f4de39a09002737dd7bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_exploration_found_good_vaccin : tensor(0.7000)\n",
      "std_exploration_found_good_vaccin : tensor(0.4606)\n",
      "E_exploitation_regrets : tensor(12.1021)\n",
      "std_exploitation_regrets : tensor(31.6172)\n",
      "std_exploitation_regrets rate : tensor(0.0632)\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "M = 500\n",
    "log = print if False else (lambda *_, **_2 : None)\n",
    "\n",
    "def compute_empirical_immuniation_rate(vaccins_used : torch.Tensor, Rk : torch.Tensor, nb_vaccin : int) -> torch.Tensor:\n",
    "    \n",
    "    n_patients : int = vaccins_used.shape[0]\n",
    "\n",
    "    vaccins_used = vaccins_used.to(torch.int)\n",
    "    log(\"vaccins_used =\", vaccins_used)\n",
    "\n",
    "    log(\"Rk =\", Rk)\n",
    "    log(\"Rk shape =\", Rk.shape)\n",
    "\n",
    "    n_patient = vaccins_used.shape[0]\n",
    "    assert(Rk.shape[0] == n_patient)\n",
    "\n",
    "    # compute the number of times each vaccin has been used\n",
    "    T = torch.bincount(vaccins_used)\n",
    "    log(\"T =\", T)\n",
    "    \n",
    "    # Compute the indicator function\n",
    "    X = torch.zeros((n_patients, nb_vaccin))\n",
    "    X[torch.arange(n_patients), vaccins_used] = 1\n",
    "    log(\"X shape =\", X.shape)\n",
    "    # log(\"X :\", X)\n",
    "\n",
    "    Ris = torch.Tensor([1 / T[i] * (X[:,i] @ Rk) for i in range(nb_vaccin)]).flatten()\n",
    "    log(Ris)\n",
    "\n",
    "    return Ris\n",
    "\n",
    "\n",
    "def exploration(MAB : List[ArmBernoulli], n_patients : int) -> tuple[torch.tensor, torch.tensor]:\n",
    "\n",
    "    nb_vaccin = len(MAB)\n",
    "\n",
    "    log(\"Arms immunity rate :\", [arm.immunity_rate for arm in MAB])\n",
    "\n",
    "    # Vaccin used for all patients \n",
    "    vaccins_used = torch.randint(0, nb_vaccin, (n_patients,))\n",
    "\n",
    "    # apply vaccins\n",
    "    Rk = torch.cat([MAB[vaccins_used[k]].sample(1) for k in range(n_patients)]).flatten().to(torch.float)\n",
    "    \n",
    "    return vaccins_used, Rk\n",
    "\n",
    "def compute_regret(n_success_exploration_vaccin : int, MAB : List[ArmBernoulli], real_best_vaccin : int, n_patients : int):\n",
    "\n",
    "    max_vaccin = (n_patients * MAB[real_best_vaccin].immunity_rate).item()\n",
    "    log(\"max_vaccin :\", max_vaccin)\n",
    "\n",
    "    return max_vaccin - n_success_exploration_vaccin\n",
    "\n",
    "def exploitation_no_update(MAB : List[ArmBernoulli], exploration_best_vaccin : int, n_patients : int) -> int:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        float: Number of patients that have been immune by a vaccin\n",
    "    \"\"\"\n",
    "\n",
    "    # apply vaccin in exploitation\n",
    "    n_success_exploration_vaccin = MAB[exploration_best_vaccin].sample(n_patients).sum().item()\n",
    "    log(\"exploration_vaccin :\", n_success_exploration_vaccin)\n",
    "\n",
    "    return n_success_exploration_vaccin\n",
    "\n",
    "def simulation(MAB : List[ArmBernoulli], n_patients_exploration : int, n_patients_exploitation : int) -> Tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Tuple[bool, float]: Is vaccin found by exploration then TRUE | Regret of the exploitation\n",
    "    \"\"\"\n",
    "               \n",
    "    vaccins_used, Rk = exploration(MAB, n_patients_exploration)\n",
    "\n",
    "    Ris = compute_empirical_immuniation_rate(vaccins_used=vaccins_used, Rk=Rk, nb_vaccin=len(MAB))\n",
    "    exploration_best_vaccin = Ris.argmax()\n",
    "    log(\"exploration best vaccin :\", exploration_best_vaccin)\n",
    "\n",
    "    real_best_vaccin = np.array([arm.immunity_rate for arm in MAB]).argmax()\n",
    "    log(\"real best vaccin :\", real_best_vaccin)\n",
    "\n",
    "    n_success_exploration_vaccin = exploitation_no_update(MAB, exploration_best_vaccin, n_patients=n_patients_exploitation)\n",
    "\n",
    "    regret = compute_regret(n_success_exploration_vaccin, MAB, real_best_vaccin, n_patients_exploitation)\n",
    "    log(\"Regret :\", regret)\n",
    "\n",
    "    return real_best_vaccin == exploration_best_vaccin, regret \n",
    "\n",
    "def print_results(samples_exploration_found_good_vaccin : torch.Tensor, exploitation_regrets : torch.Tensor) -> None:\n",
    "\n",
    "    if samples_exploration_found_good_vaccin is not None:\n",
    "        print(\"E_exploration_found_good_vaccin :\", samples_exploration_found_good_vaccin.mean())\n",
    "        print(\"std_exploration_found_good_vaccin :\", samples_exploration_found_good_vaccin.std())\n",
    "\n",
    "    print(\"E_exploitation_regrets :\", exploitation_regrets.mean())\n",
    "    std_exploitation_regrets = exploitation_regrets.std()\n",
    "    print(\"std_exploitation_regrets :\", std_exploitation_regrets)\n",
    "    print(\"std_exploitation_regrets rate :\", std_exploitation_regrets / M)\n",
    "\n",
    "if False: # test with one simulation\n",
    "    i = torch.randint(0, 100, (1, )).item()\n",
    "    print(\"index :\", i)\n",
    "    simulation(MABs[i], N, M)\n",
    "\n",
    "if True: # test with all the simulations\n",
    "    \n",
    "    l = torch.Tensor([simulation(MAB, N, M) for MAB in tqdm(MABs)])\n",
    "\n",
    "    samples_exploration_found_good_vaccin = l[:, 0]\n",
    "    exploitation_regrets = l[:, 1]\n",
    "\n",
    "    print_results(samples_exploration_found_good_vaccin, exploitation_regrets)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. On propose d'améliorer l'algorithme précédant en mettant à jour les taux d'immunisation empiriques $\\bar{R}_i$ pendant l'exploitation. Notez vous une amélioration du regret ? Proposez un exemple dans lequel cette mise à jour ne changera rien.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d858a1168a12446c9f5bef7288205191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_exploitation_regrets : tensor(3.3921)\n",
      "std_exploitation_regrets : tensor(14.1552)\n",
      "std_exploitation_regrets rate : tensor(0.0283)\n"
     ]
    }
   ],
   "source": [
    "def exploitation_update_immune_rate(MAB : List[ArmBernoulli], vaccins_used : torch.Tensor, Rk : torch.Tensor, n_patients : int) -> float:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        float: Number of patients that have been immune by a vaccin\n",
    "    \"\"\"\n",
    "\n",
    "    sum = 0\n",
    "    for _ in range(n_patients):\n",
    "\n",
    "        # Choose the vaccin considered as the best one\n",
    "        Ris = compute_empirical_immuniation_rate(vaccins_used=vaccins_used, Rk=Rk, nb_vaccin=len(MAB))\n",
    "        best_vaccin = Ris.argmax().to(torch.int)\n",
    "        log(\"exploration best vaccin :\", best_vaccin)\n",
    "\n",
    "        # apply vaccin\n",
    "        success = MAB[best_vaccin].sample(1)\n",
    "        sum += success.item()\n",
    "\n",
    "        # update vaccins_used and Rk\n",
    "        vaccins_used = torch.cat((vaccins_used, torch.Tensor([best_vaccin])))\n",
    "        Rk = torch.cat((Rk, success))\n",
    "\n",
    "    return sum\n",
    "\n",
    "def simulation(MAB : List[ArmBernoulli], n_patients_exploration : int, n_patients_exploitation : int) -> float:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        float: Regret of the exploitation\n",
    "    \"\"\"\n",
    "\n",
    "    # exploration           \n",
    "    vaccins_used, Rk = exploration(MAB, n_patients_exploration)\n",
    "\n",
    "    # exploitation\n",
    "    n_success_exploration_vaccin = exploitation_update_immune_rate(MAB, vaccins_used, Rk, n_patients_exploitation)\n",
    "\n",
    "    # compute regret\n",
    "    real_best_vaccin = np.array([arm.immunity_rate for arm in MAB]).argmax()\n",
    "    log(\"real best vaccin :\", real_best_vaccin)\n",
    "\n",
    "    regret = compute_regret(n_success_exploration_vaccin, MAB, real_best_vaccin, n_patients_exploitation)\n",
    "    log(\"Regret :\", regret)\n",
    "\n",
    "    return regret\n",
    "\n",
    "if True: # test with all the simulations\n",
    "    \n",
    "    exploitation_regrets = torch.Tensor([simulation(MAB, N, M) for MAB in tqdm(MABs)])\n",
    "\n",
    "    print_results(None, exploitation_regrets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*\n",
    "\n",
    "Cette mise à jour ne sert à rien lorsque le meilleur vaccin a été trouvé lors de l'exploration et que mettre à jour les taux d'immunsation des vaccins ne fait pas changer le vaccin qui est considéré comme le plus performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Créez une figure contenant deux sous-figures : à gauche, le taux d'immunisation empirique $\\bar{R}_i$ pour les 5 vaccins ; à droite, le regret $r_n$. La figure sera animée avec les patients : chaque frame $k$ de l'animation représente le vaccin que l'on donne au $k$-ième patient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. On étudie maintenant l'influence de la taille du training set $N$. On considère que N+M est une constante, puis on fait varier N entre K et M. Calculez le regret pour ces différentes tailles du training set  différents MAB et representez le regret moyen, le regret min et max (vous devriez trouver une courbe en U ou en V pour le regret moyen). Quelle est la taille optimale du training set ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I.b. Borne inférieure de Lai & Robbins [Lai et Robbins, 1985]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un modèle de manchot de Bernoulli (équivalent au problème étudié), la borne inférieure de Lai et Robbins [Lai et Robbins, 1985] stipule que :\n",
    "\n",
    "$$\\lim \\inf_{n\\rightarrow \\infty} \\frac{\\sum_{k=0}^{n-1} R_k}{\\log n} \\geq \\sum_{i~\\text{tel que}~\\mu_i \\lt \\mu^*} \\frac{\\mu^∗−\\mu_i}{\\text{KL}(\\mu_i, \\mu^*)}  :=C(\\mu)$$\n",
    " \n",
    " avec $\\text{KL}(x, y) = x \\log(x/y) + (1 − x) \\log((1 − x)/(1 − y))$ (distance de Kullback-Leibler) et  $\\sum_{k=0}^{n-1} R_k$ la récompense obtenue sur $n$ patients (avec un algorithme optimal). \n",
    " \n",
    " \n",
    "**Q6. Justifiez pourquoi cette borne signifie que la machine optimale est jouée exponentiellement plus souvent que les autres machines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. Tracez le regret issu de la borne de Lai & Robbins et comparez le au regret obtenu avec l'algorithme glouton.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.c. Upper Confidence Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet algorithme améliore la version précédente en ajoutant un biais lié à la fréquentation de chaque vaccin :\n",
    "\n",
    "$$\\hat{R}_i = \\bar{R}_i + \\sqrt{\\frac{C\\log{n}}{T_i}}$$,\n",
    "\n",
    "avec $C=2$.\n",
    "\n",
    "**Q8. Implémentez la modification de cette algorithme. Conservez les deux phases exploration/exploitation décrites ci-dessus. En prenant les valeurs de $N$ et $M$ trouvées à la question Q5, quel regret obtenez-vous ? Faites l'expérience avec au moins 10 MAB différents (tous ayant 5 vaccins) afin de calculer la moyenne et l'écart-type du regret.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9. Reprenez la questions Q4 avec cette algorithme. Dans la figure de gauche, vous representerez $\\bar{R}_i$ et $\\hat{R}_i$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10. Reprenez la question Q5 avec cette algorithme. Concluez sur l'utilité (ou l'inutilité) de la phase d'exploration. Comparez les performances d'UCB avec celles de l'algorithme glouton.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11. Testez différentes valeurs pour $C$ et trouvez sa valeur optimale expérimentalement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echantillonnage de Thomson\n",
    "\n",
    "Cet algorithme propose de modéliser la variable aléatoire de chaque vaccin avec une loi $\\beta$ dont les paramètres $a$ et $b$ correspondent au nombre de patients que le vaccin a immunisés (resp. non immunisés).\n",
    "\n",
    "Pour chaque patient, on tire un valeur aléatoire pour la loi $\\beta$ décrivant chaque vaccin, puis on choisit le vaccin avec la plus grande valeur tirée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12. Implémentez cet algorithme. Conservez les deux phases exploration/exploitation décrites ci-dessus. En prenant les valeurs de $N$ et $M$ trouvées à la question Q5, quel regret obtenez-vous ? Faites l'expérience avec au moins 10 MAB différents (tous ayant 5 vaccins) afin de calculer la moyenne et l'écart-type du regret.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13. Reprenez la question Q4, mais cette fois-ci, vous representerez le taux d'immunisation empirique avec un [graphique en violon](https://en.wikipedia.org/wiki/Violin_plot) qui représente la loi beta associée à chaque vaccin.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14. Représentez son regret pour différentes tailles du training set (comme dans la Q5). Comparez le regret avec les autres algorithmes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "**Q15. Calculez le regret des algorithmes glouton, UCB & Thomson lorsqu'il y a un grand nombre de vaccins disponibles (K=100) (on prendra N=100). Faites le lien avec la [malédiction de la dimension](https://fr.wikipedia.org/wiki/Fl%C3%A9au_de_la_dimension).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
